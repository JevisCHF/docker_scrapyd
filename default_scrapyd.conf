[scrapyd]
eggs_dir    = eggs
logs_dir    = logs
# item_dir为备份下载目录，一般不开启
items_dir   =
# 每个蜘蛛要完成的工作数量，默认为5
jobs_to_keep = 10
dbs_dir     = dbs
# scrapy项目的最大的运行数量，0表示核心数*，每核心线程数(max_proc_per_cpu)
max_proc    = 0
# cpu每核心启动的线程数
max_proc_per_cpu = 4
# 最多完成scrapy项目个数(超过此数目的项目就不能上传到scrapyd中)
finished_to_keep = 200
# 轮询队列的事假间隔，默认 5s 可为小数
poll_interval = 5
bind_address = 0.0.0.0
http_port   = 6800
debug       = off
runner      = scrapyd.runner
application = scrapyd.app.application
launcher    = scrapyd.launcher.Launcher
webroot     = scrapyd.website.Root

[services]
schedule.json     = scrapyd.webservice.Schedule
cancel.json       = scrapyd.webservice.Cancel
addversion.json   = scrapyd.webservice.AddVersion
listprojects.json = scrapyd.webservice.ListProjects
listversions.json = scrapyd.webservice.ListVersions
listspiders.json  = scrapyd.webservice.ListSpiders
delproject.json   = scrapyd.webservice.DeleteProject
delversion.json   = scrapyd.webservice.DeleteVersion
listjobs.json     = scrapyd.webservice.ListJobs
daemonstatus.json = scrapyd.webservice.DaemonStatus


